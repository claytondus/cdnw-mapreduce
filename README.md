Programming Assignment 2: MapReduce
===================================
Clayton Davis, Nathan Wilder
----------------------------

Requirements
------------
This project requires Python 2.7+ and the pyparsing package:

sudo pip install pyparsing



Part 1: Stop words
------------------
Generation of the stops words is a MapReduce process within this assignment. In line with the assignment guidelines, a generic stop-words list from the internet is not used. Instead a stop word list is built specific to the documents that are being indexed. The initial processing prior to any map functions is to create HDFS folders for the assignment and copy over the documents to be indexed. For this assignment we optimized parsing for pg100.txt which is the concatenated volume of all of shakespeare's works.  Extraction of titles and indvidual works are dependent on the formatting used in the document and the scripts would need to be slightly adjusted to work with other formats although it will still detect individual file names (some of the individual shakespeare documents from Gutenberg use the same formatting as the complete works, but not all do). 

Once the input data is in HDFS, the first MapReduce set of functions performs a word count per document (individual work of shakespeare found in the complete works file). This list is then merged and counted in the second MapReduce set of functions (the reduce is actually the same as the first set here) in order to provide a count of the number of documents a word appears in. In the final stop-words MapReduce set of functions, if the word appears in about 90% of the documents (51 or greater out of 38), then it is added to the stop-word list. To adjust the threshold for inclusion in the stop words list, change the global variable at the start of mapper3.py. The resulting stop-word list is of course very specific to the works being indexed. For example, the word 'king' is included when shakespeare's works are indexed, but likely would not be included if a broader set of works were indexed. In this way the stop-words list helps fine-tune the search index more so than a generic stop-word list built against all English language trends in general.

Part 2: Inverted index
----------------------
Once the stop-words list has been generated (as stop-words.json), its values are used to limit the indexing of the original input data via a final set of MapReduce functions (mapper4.py and reducer4.py).  These functions build an inverted index for the source material, filtering out stop words in the mapper function and joining multiple occurances of a word together in a JSON array in reducer.  The final index stores the title, line, and line position of each word in an inverted index that is formatted in JSON.

Both part 1 and part 2 can be run by use of the included shell script, to run first change to the 'stop-words-and-index' folder and then enter:
`./stop-words-and-index.sh`
This shell script will also prep the HDFS folders needed and has been pre-configured for the hadoop-streaming jar location on Azure's HDInsight. If running on another system, edit the first few lines of the script where the relative HDFS path is set and where the streaming jar's location is set. Also, note that some HDFS systems may require issuing the command:
`hadoop fs -mkdir -p /user/[current-username]`
if this command has not previously been issued or the resulting directory does not already exist (many Hadoop installs create this HDFS directory by default). If needed this command can be issued prior to running the shell script.

Part 3: Query
-------------
After generating the inverted index, the query program is ready for use.
This program accepts one or more word arguments (separated by spaces) and
returns the location of each occurrence, sorted by title, line number, and
starting position on the line.

To run, change to the "simplequery" directory and enter:
`./simplequery.py <term1> <term2> ...`

Terms are associated with logical OR; any index entry matching any search term
will be returned.  Results are not de-duplicated.

The simplequery.py script first loads the inverted_index.json file generated by
the last MapReduce stage.  This is processed into a Python dictionary which
contains search terms as keys, and the list of index entries as the value.
For each word, the index entries are sorted by the title, line number, and
position, and then printed to the console.

If a word does not exist in the index, no entries are returned, and processing
continues until all search words are retrieved from the index.


Part 4: Web Portal
------------------
The web portal is implemented as a Flask application which loads a Python module
containing a query parser (CdnwParser).  The parser is an extension of a class
provided from the PyParsing project, which compiles Boolean search expressions
and returns a set of results from an inverted index.  The provided code is able
to understand the "and", "or", and "not" operators, as well as parenthesis,
quote literals, and the asterisk wildcard.  Only the "or" operator is
implemented over our index, due to the complexity of mapping our index to the
parser implementation.  The parser originally used integer values contained in a
set() to represent documents containing the search terms.  Our result entries
are tuples, and each is unique, preventing the query parser from correctly

To run the web portal:
`./web/server.py`

The web server will start and display a URL where the application can be locally
accessed.  No support is provided for remote access.  

The default page displays an input form control, where a Boolean search can be
entered.  After selecting the "Enquire" button, the results are printed in a
table after the form.  An important difference between the simplequery.py syntax
and the portal's syntax is the implicit operator between adjacent search terms.
simplequery.py has an implicit operator of OR, while the web portal uses AND.

The Flask application consists of a single route, search(), mapped to "/".
When the application is first loaded, no query string is present, and only the
form is displayed.  On submitting the form, a GET request is sent to the same
route, with the query string containing the search term.  The route detects
the presence of the "search" key in the request.args MultiDict, and then calls
the QueryParser.Parse() function on the search string.  The results are returned
and saved to a view-model dictionary which can be passed to the Jinja2 template
engine.  The template engine renders the results by looping over the 'results'
set and generating table rows for each result.  A result count is prefixed to
the table by calling the 'length' filter in the Jinja2 engine.
